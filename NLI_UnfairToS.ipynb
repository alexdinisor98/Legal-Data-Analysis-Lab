{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTWf6WV4Avz0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnvJuMdKTv8G"
      },
      "outputs": [],
      "source": [
        "!pip install -U torchtext==0.10.0\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3GKrysS5YMxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFeOsu3RA-pS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETVsLT81BOSz"
      },
      "outputs": [],
      "source": [
        "id2label = {0 : \"Limitation of liability\",\n",
        "           1 : \"Unilateral termination\",\n",
        "           2: \"Unilateral change\",\n",
        "           3: \"Content removal\",\n",
        "           4: \"Contract by using\",\n",
        "           5: \"Choice of law\",\n",
        "           6: \"Jurisdiction\",\n",
        "           7: \"Arbitration\", \n",
        "          }\n",
        "\n",
        "descrip_label = {\"Limitation of liability\" : \"Limitation of liability stipulates that the duty to pay damages is limited or excluded, for certain kind of losses, under certain conditions.\",\n",
        "           \"Unilateral termination\" : \"Unilateral termination gives provider the right to suspend and/or terminate the service and/or the contract, and sometimes details the circumstances under which the provider claims to have a right to do so. Unilateral termination clauses that specify reasons for termination were marked as potentially unfair, whereas clauses stipulating that the service provider may suspend or terminate the service at any time for any or no reasons and/or without notice were marked as clearly unfair.\",\n",
        "           \"Unilateral change\": \"Unilateral change specifies the conditions under which the service provider could amend and modify the terms of service and/or the service itself.\",\n",
        "           \"Content removal\": \"Content removal  gives the provider a right to modify/delete user’s content, including in-app purchases, and sometimes specifies the conditions under which the service provider may do so.\",\n",
        "           \"Contract by using\": \"Contract by using stipulates that the consumer is bound by the terms of use of a specific service, simply by using the service, without even being required to mark that he or she has read and accepted them.\",\n",
        "           \"Choice of law\": \"Choice of law  specifies what law will govern the contract, meaning also what law will be applied in potential adjudication of a dispute arising under the contract. \",\n",
        "           \"Jurisdiction\": \"Jurisdiction stipulates what courts will have the competence to adjudicate disputes under the contract. Clauses stating that any judicial proceeding takes a residence away (i.e. in a different city, different country) were marked as clearly unfair.\",\n",
        "           \"Arbitration\": \"Arbitration requires or allows the parties to resolve their disputes through an arbitration process, before the case could go to court. It is therefore considered a kind of forum selection clause. Clauses stipulating that the arbitration should take place in a state other then the state of consumer’s residence and/or be based not on law but on arbiter’s discretion were marked as clearly unfair.\", \n",
        "           }\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4gnqIp3BQKg"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, list_datasets\n",
        "dataset = load_dataset(\"lex_glue\", 'unfair_tos')\n",
        "\n",
        "df_all = pd.DataFrame()\n",
        "df_training = pd.DataFrame()\n",
        "df_valid = pd.DataFrame()\n",
        "df_testing = pd.DataFrame()\n",
        "\n",
        "for row in dataset['train']:\n",
        "    df_all = df_all.append(row, ignore_index=True)\n",
        "    row['labels'] = [id2label[l] for l in row['labels']]\n",
        "    df_training = df_training.append(row, ignore_index=True)\n",
        "\n",
        "for row in dataset['validation']:\n",
        "    df_all = df_all.append(row, ignore_index=True)\n",
        "    row['labels'] = [id2label[l] for l in row['labels']]\n",
        "    df_valid = df_valid.append(row, ignore_index=True)\n",
        "\n",
        "for row in dataset['test']:\n",
        "    df_all = df_all.append(row, ignore_index=True)\n",
        "    row['labels'] = [id2label[l] for l in row['labels']]\n",
        "    df_testing = df_testing.append(row, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGGhLZjZFqEg"
      },
      "outputs": [],
      "source": [
        "def modif_df(df_test, test=False):\n",
        "  df_modif_test = pd.DataFrame(columns=['gold_label','sentence1', 'sentence2'])\n",
        "\n",
        "  for index, row in df_test.iterrows():\n",
        "      for label in list(descrip_label.values()):\n",
        "        if label in row['labels']:\n",
        "            df_modif_test = df_modif_test.append({'gold_label': '1', 'sentence1': row['text'] , 'sentence2': label}, ignore_index=True)\n",
        "        else:\n",
        "            df_modif_test = df_modif_test.append({'gold_label': '0', 'sentence1': row['text'], 'sentence2': label}, ignore_index=True)\n",
        "\n",
        "  return df_modif_test\n",
        "\n",
        "df_modif_test = modif_df(df_testing)\n",
        "df_modif_train = modif_df(df_training)\n",
        "df_modif_val = modif_df(df_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3YAJ5HQaE4U"
      },
      "outputs": [],
      "source": [
        "df_modif_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_modif_test))\n",
        "df_modif_test['gold_label'][0]"
      ],
      "metadata": {
        "id": "CuVFFV588Fxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_true_test = pd.DataFrame(columns=['Limitation_of_liability', 'Unilateral_termination', 'Unilateral_change',\n",
        "                                      'Content_removal', 'Contract_by_using', 'Choice_of_law', 'Jurisdiction', 'Arbitration', 'No_violation'])\n",
        "\n",
        "for i in range(0, len(df_modif_test), 8):\n",
        "    if df_modif_test['gold_label'][i] == '0' and  df_modif_test['gold_label'][i + 1] == '0' and df_modif_test['gold_label'][i + 2] == '0' and df_modif_test['gold_label'][i + 3] == '0' and df_modif_test['gold_label'][i + 4] == '0' and df_modif_test['gold_label'][i + 5] == '0' and df_modif_test['gold_label'][i + 6] == '0' and df_modif_test['gold_label'][i + 7] == '0':\n",
        "        df_true_test = df_true_test.append({\n",
        "            # 'sentence1': df_modif_test['sentence1'][i] ,\n",
        "                                            'Limitation_of_liability': df_modif_test['gold_label'][i], \n",
        "                                            'Unilateral_termination': df_modif_test['gold_label'][i + 1],\n",
        "                            'Unilateral_change': df_modif_test['gold_label'][i + 2], 'Content_removal': df_modif_test['gold_label'][i + 3], 'Contract_by_using': df_modif_test['gold_label'][i + 4], \n",
        "                            'Choice_of_law': df_modif_test['gold_label'][i + 5], 'Jurisdiction': df_modif_test['gold_label'][i + 6], 'Arbitration': df_modif_test['gold_label'][i + 7],\n",
        "                            'No_violation' : '1'}\n",
        "                            , ignore_index=True)\n",
        "    else:\n",
        "        df_true_test = df_true_test.append({\n",
        "            # 'sentence1': df_modif_test['sentence1'][i] ,\n",
        "                                            'Limitation_of_liability': df_modif_test['gold_label'][i], \n",
        "                                            'Unilateral_termination': df_modif_test['gold_label'][i + 1],\n",
        "                            'Unilateral_change': df_modif_test['gold_label'][i + 2], 'Content_removal': df_modif_test['gold_label'][i + 3], 'Contract_by_using': df_modif_test['gold_label'][i + 4], \n",
        "                            'Choice_of_law': df_modif_test['gold_label'][i + 5], 'Jurisdiction': df_modif_test['gold_label'][i + 6], 'Arbitration': df_modif_test['gold_label'][i + 7],\n",
        "                            'No_violation' : 'no'}\n",
        "                            , ignore_index=True)\n",
        "\n",
        "df_true_test.head()"
      ],
      "metadata": {
        "id": "khNL5Lu86Rpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_true_test = df_true_test.apply(pd.to_numeric)\n",
        "# df_true_test['all'] = df_true_test.apply(', '.join, axis=1)\n",
        "df_true_test"
      ],
      "metadata": {
        "id": "znK-kWDPCJ3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW5VZZmEBkqa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "SEED = 1111\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLD01GYrCA-H"
      },
      "outputs": [],
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10bn2y6QCFHg"
      },
      "outputs": [],
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYL5r_LNCHsS"
      },
      "outputs": [],
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)\n",
        "\n",
        "max_input_length = 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnA-jDQhDUe9"
      },
      "source": [
        "##Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q-Y92iBCKWk"
      },
      "outputs": [],
      "source": [
        "def tokenize_bert(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    return tokens\n",
        "\n",
        "def split_and_cut(sentence):\n",
        "    tokens = sentence.strip().split(\" \")\n",
        "    tokens = tokens[:max_input_length-1]\n",
        "    return tokens\n",
        "    \n",
        "def trim_sentence(sent):\n",
        "    try:\n",
        "        sent = sent.split()\n",
        "        sent = sent[:128]\n",
        "        return \" \".join(sent)\n",
        "    except:\n",
        "        return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycktCKS2DWjv"
      },
      "outputs": [],
      "source": [
        "def get_sent1_token_type(sent):\n",
        "    try:\n",
        "        return [0]* len(sent)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def get_sent2_token_type(sent):\n",
        "    try:\n",
        "        return [1]* len(sent)\n",
        "    except:\n",
        "        return []\n",
        "    \n",
        "def combine_seq(seq):\n",
        "    return \" \".join(seq)\n",
        "\n",
        "def combine_mask(mask):\n",
        "    mask = [str(m) for m in mask]\n",
        "    return \" \".join(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyKgk2nxO7gH"
      },
      "outputs": [],
      "source": [
        "df_modif_train.loc[264:271]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftKjIMjtDYaS"
      },
      "outputs": [],
      "source": [
        "df_train = df_modif_train[['gold_label','sentence1','sentence2']]\n",
        "df_dev = df_modif_val[['gold_label','sentence1','sentence2']]\n",
        "df_test = df_modif_test[['gold_label','sentence1','sentence2']]\n",
        "\n",
        "df_train['sentence1'] = df_train['sentence1'].apply(trim_sentence)\n",
        "df_train['sentence2'] = df_train['sentence2'].apply(trim_sentence)\n",
        "df_dev['sentence1'] = df_dev['sentence1'].apply(trim_sentence)\n",
        "df_dev['sentence2'] = df_dev['sentence2'].apply(trim_sentence)\n",
        "df_test['sentence1'] = df_test['sentence1'].apply(trim_sentence)\n",
        "df_test['sentence2'] = df_test['sentence2'].apply(trim_sentence)\n",
        "\n",
        "df_train['sent1'] = '[CLS] ' + df_train['sentence1'] + ' [SEP] '\n",
        "df_train['sent2'] = df_train['sentence2'] + ' [SEP]'\n",
        "df_dev['sent1'] = '[CLS] ' + df_dev['sentence1'] + ' [SEP] '\n",
        "df_dev['sent2'] = df_dev['sentence2'] + ' [SEP]'\n",
        "df_test['sent1'] = '[CLS] ' + df_test['sentence1'] + ' [SEP] '\n",
        "df_test['sent2'] = df_test['sentence2'] + ' [SEP]'\n",
        "\n",
        "df_train['sent1_t'] = df_train['sent1'].apply(tokenize_bert)\n",
        "df_train['sent2_t'] = df_train['sent2'].apply(tokenize_bert)\n",
        "df_dev['sent1_t'] = df_dev['sent1'].apply(tokenize_bert)\n",
        "df_dev['sent2_t'] = df_dev['sent2'].apply(tokenize_bert)\n",
        "df_test['sent1_t'] = df_test['sent1'].apply(tokenize_bert)\n",
        "df_test['sent2_t'] = df_test['sent2'].apply(tokenize_bert)\n",
        "\n",
        "df_train['sent1_token_type'] = df_train['sent1_t'].apply(get_sent1_token_type)\n",
        "df_train['sent2_token_type'] = df_train['sent2_t'].apply(get_sent2_token_type)\n",
        "df_dev['sent1_token_type'] = df_dev['sent1_t'].apply(get_sent1_token_type)\n",
        "df_dev['sent2_token_type'] = df_dev['sent2_t'].apply(get_sent2_token_type)\n",
        "df_test['sent1_token_type'] = df_test['sent1_t'].apply(get_sent1_token_type)\n",
        "df_test['sent2_token_type'] = df_test['sent2_t'].apply(get_sent2_token_type)\n",
        "\n",
        "df_train['sequence'] = df_train['sent1_t'] + df_train['sent2_t']\n",
        "df_dev['sequence'] = df_dev['sent1_t'] + df_dev['sent2_t']\n",
        "df_test['sequence'] = df_test['sent1_t'] + df_test['sent2_t']\n",
        "\n",
        "\n",
        "df_train['attention_mask'] = df_train['sequence'].apply(get_sent2_token_type)\n",
        "df_dev['attention_mask'] = df_dev['sequence'].apply(get_sent2_token_type)\n",
        "df_test['attention_mask'] = df_test['sequence'].apply(get_sent2_token_type)\n",
        "\n",
        "df_train['token_type'] = df_train['sent1_token_type'] + df_train['sent2_token_type']\n",
        "df_dev['token_type'] = df_dev['sent1_token_type'] + df_dev['sent2_token_type']\n",
        "df_test['token_type'] = df_test['sent1_token_type'] + df_test['sent2_token_type']\n",
        "\n",
        "df_train['sequence'] = df_train['sequence'].apply(combine_seq)\n",
        "df_dev['sequence'] = df_dev['sequence'].apply(combine_seq)\n",
        "df_test['sequence'] = df_test['sequence'].apply(combine_seq)\n",
        "\n",
        "df_train['attention_mask'] = df_train['attention_mask'].apply(combine_mask)\n",
        "df_dev['attention_mask'] = df_dev['attention_mask'].apply(combine_mask)\n",
        "df_test['attention_mask'] = df_test['attention_mask'].apply(combine_mask)\n",
        "\n",
        "df_train['token_type'] = df_train['token_type'].apply(combine_mask)\n",
        "df_dev['token_type'] = df_dev['token_type'].apply(combine_mask)\n",
        "df_test['token_type'] = df_test['token_type'].apply(combine_mask)\n",
        "\n",
        "df_train = df_train[['gold_label', 'sequence', 'attention_mask', 'token_type']]\n",
        "df_dev = df_dev[['gold_label', 'sequence', 'attention_mask', 'token_type']]\n",
        "df_test = df_test[['gold_label', 'sequence', 'attention_mask', 'token_type']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1blCOV9RElj"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.loc[df_train['gold_label'].isin(['1','0'])]\n",
        "df_dev = df_dev.loc[df_dev['gold_label'].isin(['1','0'])]\n",
        "df_test = df_test.loc[df_test['gold_label'].isin(['1','0'])]\n",
        "\n",
        "df_train.to_csv('/unfairtos_train.csv', index=False)\n",
        "df_dev.to_csv('/unfairtos_dev.csv', index=False)\n",
        "df_test.to_csv('/unfairtos_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mpz5HFyRzyA"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgrAxNVuSCGR"
      },
      "outputs": [],
      "source": [
        "def convert_to_int(tok_ids):\n",
        "    tok_ids = [int(x) for x in tok_ids]\n",
        "    return tok_ids\n",
        "\n",
        "from torchtext.legacy.data import Field, LabelField\n",
        "\n",
        "TEXT = Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = split_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = LabelField()\n",
        "\n",
        "ATTENTION = Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = split_and_cut,\n",
        "                  preprocessing = convert_to_int,\n",
        "                  pad_token = pad_token_idx)\n",
        "\n",
        "TTYPE = Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = split_and_cut,\n",
        "                  preprocessing = convert_to_int,\n",
        "                  pad_token = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2i-8-NUSIKx"
      },
      "outputs": [],
      "source": [
        "fields = [('label', LABEL), ('sequence', TEXT), ('attention_mask', ATTENTION), ('token_type', TTYPE)]\n",
        "\n",
        "train_data, valid_data, test_data = TabularDataset.splits(\n",
        "                                        path = '/',\n",
        "                                        train = 'unfairtos_train.csv',\n",
        "                                        validation = 'unfairtos_dev.csv',\n",
        "                                        test = 'unfairtos_test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBI0UU77X6RJ"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of training data: {len(train_data)}\")\n",
        "print(f\"Number of validation data: {len(valid_data)}\")\n",
        "print(f\"Number of testing data: {len(test_data)}\")\n",
        "\n",
        "train_data_len = len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGu4wPv6YKdO"
      },
      "outputs": [],
      "source": [
        "LABEL.build_vocab(train_data)\n",
        "print(LABEL.vocab.stoi)\n",
        "print(LABEL.vocab.freqs.most_common())\n",
        "print(LABEL.vocab.itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTL6-CpgYRDu"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.sequence),\n",
        "    sort_within_batch = False, \n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsX9V1tIYX3B"
      },
      "outputs": [],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QATNvIOXYW0w"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDWAS4kNYa2O"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTNLIModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_model,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                ):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert_model\n",
        "        \n",
        "        embedding_dim = bert_model.config.to_dict()['hidden_size']\n",
        "        \n",
        "        #self.fc = nn.Linear(embedding_dim, hidden_dim)\n",
        "\n",
        "        #self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.out = nn.Linear(embedding_dim, output_dim)\n",
        "        \n",
        "        \n",
        "    def forward(self, sequence, attn_mask, token_type):\n",
        "        \n",
        "        #sequence = [sequence len, batch_size]\n",
        "        #attention_mask = [seq_len, batch_size]\n",
        "        #token_type = [seq_len, batch_size]\n",
        "                \n",
        "        embedded = self.bert(input_ids = sequence, attention_mask = attn_mask, token_type_ids= token_type)[1]\n",
        "        #print('emb ', embedded.size())\n",
        "\n",
        "        #self.bert() gives tuple which contains hidden outut corresponding to each token.\n",
        "        #self.bert()[0] = [seq_len, batch_size, emd_dim]\n",
        "                \n",
        "        #embedded = [batch size, emb dim]\n",
        "        \n",
        "        #ff = self.fc(embedded)\n",
        "        #ff = [batch size, hid dim]\n",
        "\n",
        "        #ff1 = self.fc2(ff)\n",
        "                \n",
        "        \n",
        "        \n",
        "        output = self.out(embedded)\n",
        "        #print('output: ', output.size())\n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xqGN1PYYeOw"
      },
      "outputs": [],
      "source": [
        "HIDDEN_DIM = 512\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "\n",
        "model = BERTNLIModel(bert_model,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                        ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZT2Tia_Yf3y",
        "outputId": "199feecb-58c7-4a51-fa93-11651c3b4e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 109,483,009 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqshp36aYkKY"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import transformers\n",
        "#optimizer = optim.Adam(model.parameters())\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "def get_scheduler(optimizer, warmup_steps):\n",
        "    scheduler = transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
        "    return scheduler\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = (max_preds.squeeze(1)==y).float()\n",
        "    return correct.sum() / len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWQK2Y8XY40P"
      },
      "outputs": [],
      "source": [
        "max_grad_norm = 1\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, scheduler):\n",
        "    #print(iterator)\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "        optimizer.zero_grad() # clear gradients first\n",
        "        torch.cuda.empty_cache() # releases all unoccupied cached memory \n",
        "        \n",
        "\n",
        "        sequence = batch.sequence\n",
        "        attn_mask = batch.attention_mask\n",
        "        token_type = batch.token_type\n",
        "        #print(sequence.size(), attn_mask.size(), token_type.size())\n",
        "        #print(sequence[0])\n",
        "        #print(attn_mask[0])\n",
        "        #print(token_type[0])\n",
        "        label = batch.label\n",
        "        \n",
        "        predictions = model(sequence, attn_mask, token_type)\n",
        "        \n",
        "        #predictions = [batch_size, 3]\n",
        "        #print(predictions.size())\n",
        "        \n",
        "        loss = criterion(predictions, label)\n",
        "        \n",
        "        acc = categorical_accuracy(predictions, label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5HWuzAiY61C"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    #print(iterator)\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "            #print(batch)\n",
        "\n",
        "            sequence = batch.sequence\n",
        "            attn_mask = batch.attention_mask\n",
        "            token_type = batch.token_type\n",
        "            labels = batch.label\n",
        "                        \n",
        "            predictions = model(sequence, attn_mask, token_type)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "                \n",
        "            acc = categorical_accuracy(predictions, labels)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNde4Y1UY8zN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQAY9151Y-9N"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "N_EPOCHS = 3\n",
        "\n",
        "warmup_percent = 0.2\n",
        "total_steps = math.ceil(N_EPOCHS*train_data_len*1./BATCH_SIZE)\n",
        "warmup_steps = int(total_steps*warmup_percent)\n",
        "scheduler = get_scheduler(optimizer, warmup_steps)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, scheduler)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'bert-nli.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJFGysz3ZW_m"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('bert-nli.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(model, 'text_descriplabel.pkl')"
      ],
      "metadata": {
        "id": "MB2fLxYdW_ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load('/content/text_descriplabel.pkl')"
      ],
      "metadata": {
        "id": "rzx8_LjzEAyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIp7_oS8ZY9G"
      },
      "outputs": [],
      "source": [
        "def predict_inference(premise, hypothesis, model, device):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    premise = '[CLS] ' + premise + ' [SEP]'\n",
        "    hypothesis = hypothesis + ' [SEP]'\n",
        "    \n",
        "    prem_t = tokenize_bert(premise)\n",
        "    hypo_t = tokenize_bert(hypothesis)\n",
        "    \n",
        "    #print(len(prem_t), len(hypo_t))\n",
        "    \n",
        "    prem_type = get_sent1_token_type(prem_t)\n",
        "    hypo_type = get_sent2_token_type(hypo_t)\n",
        "    \n",
        "    #print(len(prem_type), len(hypo_type))\n",
        "    \n",
        "    indexes = prem_t + hypo_t\n",
        "    \n",
        "    indexes = tokenizer.convert_tokens_to_ids(indexes)\n",
        "    #print(indexes)\n",
        "    indexes_type = prem_type + hypo_type\n",
        "    #print(indexes_type)\n",
        "    \n",
        "    attn_mask = get_sent2_token_type(indexes)\n",
        "    #print(attn_mask)\n",
        "    \n",
        "    #print(len(indexes))\n",
        "    #print(len(indexes_type))\n",
        "    #print(len(attn_mask))\n",
        "\n",
        "    #seq = '[CLS] '+ premise + ' [SEP] '+ hypothesis \n",
        "\n",
        "    #tokens = tokenizer.tokenize(seq)\n",
        "\n",
        "    #indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    \n",
        "    indexes = torch.LongTensor(indexes).unsqueeze(0).to(device)\n",
        "    indexes_type = torch.LongTensor(indexes_type).unsqueeze(0).to(device)\n",
        "    attn_mask = torch.LongTensor(attn_mask).unsqueeze(0).to(device)\n",
        "    \n",
        "    \n",
        "    prediction = model(indexes, attn_mask, indexes_type)\n",
        "    \n",
        "    prediction = prediction.argmax(dim=-1).item()\n",
        "    \n",
        "    return LABEL.vocab.itos[prediction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSsg8WvjmEqg"
      },
      "outputs": [],
      "source": [
        "# premise = 'academia.edu reserves the right , at its sole discretion , to discontinue or terminate the site and services and to terminate these terms , at any time and without prior notice . '\n",
        "# hypothesis = 'Unilateral termination' \n",
        "# print(predict_inference(premise, hypothesis, model, device))\n",
        "\n",
        "# premise = 'by using amazon services , you agree to these conditions . '\n",
        "# hypothesis = 'Contract by using'\n",
        "# print(predict_inference(premise, hypothesis, model, device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erGYmQIGiAds"
      },
      "outputs": [],
      "source": [
        "final_df = pd.DataFrame(columns=['pred', 'true', 'label'])\n",
        "\n",
        "for index, row in df_modif_test.iterrows():\n",
        "  pred = predict_inference(row['sentence1'], row['sentence2'], model, device)\n",
        "  final_df.loc[len(final_df.index)] = [pred, row['gold_label'], row['sentence2'], ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mHI9d0ulwlu"
      },
      "outputs": [],
      "source": [
        "final_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred_test = pd.DataFrame(columns=['Limitation_of_liability', 'Unilateral_termination', 'Unilateral_change',\n",
        "                                      'Content_removal', 'Contract_by_using', 'Choice_of_law', 'Jurisdiction', 'Arbitration', 'No_violation'])\n",
        "\n",
        "for i in range(0, len(final_df), 8):\n",
        "  if final_df['pred'][i] == '0' and  final_df['pred'][i + 1] == '0' and final_df['pred'][i + 2] == '0' and final_df['pred'][i + 3] == '0' and final_df['pred'][i + 4] == '0' and final_df['pred'][i + 5] == '0' and final_df['pred'][i + 6] == '0' and final_df['pred'][i + 7] == '0':      \n",
        "      df_pred_test = df_pred_test.append({\n",
        "        # 'sentence1': df_modif_test['sentence1'][i] ,\n",
        "                         'Limitation_of_liability': final_df['pred'][i], 'Unilateral_termination': final_df['pred'][i + 1],\n",
        "                         'Unilateral_change': final_df['pred'][i + 2], 'Content_removal': final_df['pred'][i + 3], 'Contract_by_using': final_df['pred'][i + 4], \n",
        "                         'Choice_of_law': final_df['pred'][i + 5], 'Jurisdiction': final_df['pred'][i + 6], 'Arbitration': final_df['pred'][i + 7],\n",
        "                          'No_violation': '1'}\n",
        "                        , ignore_index=True)\n",
        "  else:\n",
        "      df_pred_test = df_pred_test.append({\n",
        "        # 'sentence1': df_modif_test['sentence1'][i] ,\n",
        "                         'Limitation_of_liability': final_df['pred'][i], 'Unilateral_termination': final_df['pred'][i + 1],\n",
        "                         'Unilateral_change': final_df['pred'][i + 2], 'Content_removal': final_df['pred'][i + 3], 'Contract_by_using': final_df['pred'][i + 4], \n",
        "                         'Choice_of_law': final_df['pred'][i + 5], 'Jurisdiction': final_df['pred'][i + 6], 'Arbitration': final_df['pred'][i + 7],\n",
        "                          'No_violation': '0'}\n",
        "                        , ignore_index=True)\n",
        "      \n",
        "\n",
        "df_pred_test"
      ],
      "metadata": {
        "id": "GtvQFuyV9mgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred_test = df_pred_test.apply(pd.to_numeric)\n",
        "df_pred_test"
      ],
      "metadata": {
        "id": "hb4k-JSvB3vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6PwtKxel3la"
      },
      "outputs": [],
      "source": [
        "# Evaluate performance\n",
        "from sklearn import metrics\n",
        "test_preds = df_pred_test.to_numpy()\n",
        "test_targets = df_true_test.to_numpy()\n",
        "\n",
        "f1_score_micro = metrics.f1_score(test_targets, test_preds, average='micro')\n",
        "f1_score_macro = metrics.f1_score(test_targets, test_preds, average='macro')\n",
        "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97p4rtzPRY5K"
      },
      "outputs": [],
      "source": [
        "for c in ['Limitation_of_liability', 'Unilateral_termination', 'Unilateral_change',\n",
        "                                      'Content_removal', 'Contract_by_using', 'Choice_of_law', 'Jurisdiction', 'Arbitration', 'No_violation']:\n",
        "\n",
        "  test_preds = df_pred_test[c].to_numpy()\n",
        "  test_true = df_true_test[c].to_numpy()\n",
        "\n",
        "  f1_score_micro = metrics.f1_score(test_targets, test_preds, average='micro')\n",
        "  f1_score_macro = metrics.f1_score(test_targets, test_preds, average='macro')\n",
        "  print(f\"F1 Score (Micro) {c} = {f1_score_micro}\")\n",
        "  print(f\"F1 Score (Macro) {c} = {f1_score_macro}\")\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "NLI_UnfairToS.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}