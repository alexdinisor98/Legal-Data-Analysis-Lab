{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-Ycz13hbUbC"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext==0.10.0\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from pprint import pprint\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "'''\n",
        "# Seeding for reproducible results everytime\n",
        "SEED = 777\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrNraUABrDq2"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBaNhzG8dfue"
      },
      "outputs": [],
      "source": [
        "id2label = {0 : \"Limitation of liability\",\n",
        "           1 : \"Unilateral termination\",\n",
        "           2: \"Unilateral change\",\n",
        "           3: \"Content removal\",\n",
        "           4: \"Contract by using\",\n",
        "           5: \"Choice of law\",\n",
        "           6: \"Jurisdiction\",\n",
        "           7: \"Arbitration\", }\n",
        "\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Da1d8Pb-p4"
      },
      "outputs": [],
      "source": [
        "spacy_english = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leVROD_6qz16",
        "outputId": "c89db76f-056b-47c7-bd96-dcde7ab790f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'machine', 'learning']\n"
          ]
        }
      ],
      "source": [
        "def tokenize_english(text):\n",
        "  return [token.text for token in spacy_english.tokenizer(text)]\n",
        "\n",
        "### Sample Run ###\n",
        "\n",
        "sample_text = \"I love machine learning\"\n",
        "print(tokenize_english(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn8CDZ1ssIju"
      },
      "outputs": [],
      "source": [
        "from torchtext.legacy import data\n",
        "# SRC = unfair sentence\n",
        "SRC = Field(tokenize = tokenize_english, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "# TRG = labels as a single string with comma as delimiter\n",
        "TRG = Field(tokenize = tokenize_english, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "fields = [('src', SRC), ('trg', TRG)]\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "                                        path = '/content/',\n",
        "                                        train = 'train_alphabetical.csv',\n",
        "                                        validation = 'val.csv',\n",
        "                                        test = 'test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True\n",
        ")\n",
        "\n",
        "# build the vocabulary for the source (TEXT) and target (LABELS) languages\n",
        "SRC.build_vocab(train_data, max_size=10000, min_freq = 2)\n",
        "TRG.build_vocab(train_data, max_size=10000)\n",
        "\n",
        "print(f\"Unique tokens in source (text) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (label) vocabulary: {len(TRG.vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiRsZjvEME18"
      },
      "outputs": [],
      "source": [
        "# dir(english.vocab)\n",
        "\n",
        "print(TRG.vocab.__dict__.keys())\n",
        "print(list(TRG.vocab.__dict__.values()))\n",
        "e = list(TRG.vocab.__dict__.values())\n",
        "for i in e:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhhJy36TM4SV"
      },
      "outputs": [],
      "source": [
        "word_2_idx = dict(e[3])\n",
        "idx_2_word = {}\n",
        "for k,v in word_2_idx.items():\n",
        "  idx_2_word[v] = k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvt8AUrWvbA_"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
        "\n",
        "print(train_data[5].__dict__.keys())\n",
        "pprint(train_data[5].__dict__.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gmz5adIwbwF"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
        "                                                                      batch_size = BATCH_SIZE, \n",
        "                                                                      sort_within_batch=True,\n",
        "                                                                      sort_key=lambda x: len(x.src),\n",
        "                                                                      device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vWNHTlL8nSg"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "max_len_lbl = []\n",
        "max_len_text = []\n",
        "for data in train_data:\n",
        "  max_len_text.append(len(data.src))\n",
        "  max_len_lbl.append(len(data.trg))\n",
        "  if count < 10 :\n",
        "    print(\"Text - \",*data.src, \" Length - \", len(data.src))\n",
        "    print(\"Labels - \",*data.trg, \" Length - \", len(data.trg))\n",
        "    print()\n",
        "  count += 1\n",
        "\n",
        "print(\"Maximum Length of Lbl sentence {} and Text sentence {} in the dataset\".format(max(max_len_lbl),max(max_len_text)))\n",
        "print(\"Minimum Length of Lbl sentence {} and Text sentence {} in the dataset\".format(min(max_len_lbl),min(max_len_text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYL8BmZI0Bzh"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for data in train_iterator:\n",
        "  if count < 1 :\n",
        "    print(\"Shapes\", data.src.shape, data.trg.shape)\n",
        "    print()\n",
        "    print(\"Text - \",*data.src, \" Length - \", len(data.src))\n",
        "    print()\n",
        "    print(\"Label - \",*data.trg, \" Length - \", len(data.trg))\n",
        "    temp_text = data.src\n",
        "    temp_label= data.trg\n",
        "    count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr7ue9mDRNLp"
      },
      "outputs": [],
      "source": [
        "temp_label_idx = (temp_label).cpu().detach().numpy()\n",
        "temp_text_idx = (temp_text).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dg028v3Ru7c"
      },
      "outputs": [],
      "source": [
        "df_label_idx = pd.DataFrame(data = temp_label_idx, columns = [str(\"S_\")+str(x) for x in np.arange(1, 33)])\n",
        "df_label_idx.index.name = 'Time Steps'\n",
        "df_label_idx.index = df_label_idx.index + 1 \n",
        "df_label_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtkVuHkaT7ba"
      },
      "outputs": [],
      "source": [
        "df_label_word = pd.DataFrame(columns = [str(\"S_\")+str(x) for x in np.arange(1, 33)])\n",
        "df_label_word = df_label_idx.replace(idx_2_word)\n",
        "df_label_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vlOtY31y40q"
      },
      "outputs": [],
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.tag = True\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    \n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "  # Shape of x (26, 32) [Sequence_length, batch_size]\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    \n",
        "    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "    return hidden_state, cell_state\n",
        "\n",
        "input_size_encoder = len(SRC.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "print(encoder_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnGwwU6p2Zfh"
      },
      "outputs": [],
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # Shape of x (32) [batch_size]\n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "\n",
        "    # Shape of x (1, 32) [1, batch_size]\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "\n",
        "    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "    predictions = self.fc(outputs)\n",
        "\n",
        "    # Shape --> predictions (32, 4556) [batch_size , output_size]\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden_state, cell_state\n",
        "\n",
        "input_size_decoder = len(TRG.vocab)\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = 0.5\n",
        "output_size = len(TRG.vocab)\n",
        "\n",
        "decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
        "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
        "print(decoder_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0rHNbZe7ALr"
      },
      "outputs": [],
      "source": [
        "for batch in train_iterator:\n",
        "  print(batch.src.shape)\n",
        "  print(batch.trg.shape)\n",
        "  break\n",
        "\n",
        "x = batch.trg[1]\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuHGodQe4r9v"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.Encoder_LSTM = Encoder_LSTM\n",
        "    self.Decoder_LSTM = Decoder_LSTM\n",
        "\n",
        "  def forward(self, source, target, tfr=0.5):\n",
        "    # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n",
        "    batch_size = source.shape[1]\n",
        "\n",
        "    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(TRG.vocab)\n",
        "    \n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n",
        "    hidden_state, cell_state = self.Encoder_LSTM(source)\n",
        "\n",
        "    # Shape of x (32 elements)\n",
        "    x = target[0] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      # Shape --> output (32, 5766) \n",
        "      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
        "      outputs[i] = output\n",
        "      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
        "      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "\n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOQL9vk49H2U"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = TRG.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpsjQCsZ_srZ"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtH0Bnq3qFmd"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, SRC, TRG, device, max_length=50):\n",
        "    spacy_ger = spacy.load('en_core_web_sm')\n",
        "\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, SRC.init_token)\n",
        "    tokens.append(SRC.eos_token)\n",
        "    text_to_indices = [SRC.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "\n",
        "    outputs = [TRG.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == TRG.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [TRG.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "def bleu(data, model, SRC, TRG, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, SRC, TRG, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
        "    print('saving')\n",
        "    print()\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, '/content/checkpoint-NMT')\n",
        "    torch.save(model.state_dict(),'/content/checkpoint-NMT-SD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4jLBPRD9osT"
      },
      "outputs": [],
      "source": [
        "epoch_loss = 0.0\n",
        "num_epochs = 100\n",
        "best_loss = 999999\n",
        "best_epoch = -1\n",
        "sentence1 = '''you agree that this agreement and the relationship between you and \n",
        "linden lab shall be governed by the laws of the state of california without regard \n",
        "to conflict of law principles or the united nations convention on the \n",
        "international sale of goods .   '''\n",
        "\n",
        "ts1  = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
        "  model.eval()\n",
        "  translated_sentence1 = translate_sentence(model, sentence1, SRC, TRG, device, max_length=50)\n",
        "  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n",
        "  ts1.append(translated_sentence1)\n",
        "\n",
        "  model.train(True)\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    input = batch.src.to(device)\n",
        "    target = batch.trg.to(device)\n",
        "\n",
        "    # Pass the input and target for model's forward method\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    # Clear the accumulating gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss value for every epoch\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient value is it exceeds > 1\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Update the weights values using the gradients we calculated using bp \n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "    epoch_loss += loss.item()\n",
        "    writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    best_epoch = epoch\n",
        "    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
        "    if ((epoch - best_epoch) >= 10):\n",
        "      print(\"no improvement in 10 epochs, break\")\n",
        "      break\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n",
        "  \n",
        "print(epoch_loss / len(train_iterator))\n",
        "\n",
        "score = bleu(test_data[1:100], model, SRC, TRG, device)\n",
        "print(f\"Bleu score {score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yyYsPQ2ml7Y"
      },
      "source": [
        "# 11. Seq2Seq Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8seg8haidFT"
      },
      "outputs": [],
      "source": [
        "progress  = []\n",
        "import nltk\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "for i, sen in enumerate(ts1):\n",
        "  progress.append(TreebankWordDetokenizer().detokenize(sen))\n",
        "print(progress)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH2U-LbhH1dw"
      },
      "outputs": [],
      "source": [
        "progress_df = pd.DataFrame(data = progress, columns=['Predicted_Sentence'])\n",
        "progress_df.index.name = \"Epochs\"\n",
        "progress_df.to_csv('/content/predicted_sentence.csv')\n",
        "progress_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxcYB6cRJKIZ"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "# test_sentences  = [\"academia.edu reserves the right , at its sole discretion , to discontinue or terminate the site and services and to terminate these terms , at any time and without prior notice . \", \"by using amazon services , you agree to these conditions . \"]\n",
        "# actual_sentences  = [\"Unilateral termination\", \"Contract by using\"]\n",
        "val_df = pd.read_csv('/content/val.csv')\n",
        "test_sentences = val_df['text'].to_numpy()\n",
        "actual_sentences = val_df['labels'].to_numpy()\n",
        "\n",
        "pred_sentences = []\n",
        "\n",
        "final_df = pd.DataFrame(columns=['Generated_Text', 'Actual_Text'])\n",
        "# use validation set\n",
        "\n",
        "for idx, i in enumerate(test_sentences):\n",
        "  model.eval()\n",
        "  translated_sentence = translate_sentence(model, i, SRC, TRG, device, max_length=50)\n",
        "  progress.append(TreebankWordDetokenizer().detokenize(translated_sentence))\n",
        "  # print(\"Text : {}\".format(i))\n",
        "  # print(\"Actual Label: {}\".format(actual_sentences[idx]))\n",
        "  # print(\"Predicted Label : {}\".format(progress[-1]))\n",
        "  print(idx)\n",
        "  final_df.loc[len(final_df.index)] = [progress[-1], actual_sentences[idx]]\n",
        "  # print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfS3jqyLs4pT"
      },
      "outputs": [],
      "source": [
        "final_df.head(300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-l7drsks8je"
      },
      "outputs": [],
      "source": [
        "id2label = {0 : \"Limitation of liability\",\n",
        "           1 : \"Unilateral termination\",\n",
        "           2: \"Unilateral change\",\n",
        "           3: \"Content removal\",\n",
        "           4: \"Contract by using\",\n",
        "           5: \"Choice of law\",\n",
        "           6: \"Jurisdiction\",\n",
        "           7: \"Arbitration\", \n",
        "           8: \"No violation\"}\n",
        "\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpG_D5zPs-Sa"
      },
      "outputs": [],
      "source": [
        "modif_final_df = pd.DataFrame(columns = ['Generated_Text', 'Actual_Text'])\n",
        "def clean_pred(row_pred):\n",
        "  cleaned_pred = ''\n",
        "  lower_id2label = list(map(lambda x: x.lower(), id2label.values()))\n",
        "  for predefined_label in list(lower_id2label):\n",
        "    if str(row_pred).find(predefined_label) > -1:\n",
        "      cleaned_pred += predefined_label + ', '\n",
        "  return cleaned_pred[:-2]\n",
        "\n",
        "modif_final_df['Generated_Text'] = final_df['Generated_Text'].apply(clean_pred)\n",
        "modif_final_df['Actual_Text'] = final_df['Actual_Text'].apply(lambda x: x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyvpp3PRs_gk"
      },
      "outputs": [],
      "source": [
        "modif_final_df.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZUaruOdtAXq"
      },
      "outputs": [],
      "source": [
        "# Evaluate performance\n",
        "from sklearn import metrics\n",
        "val_preds = modif_final_df['Generated_Text'].to_numpy()\n",
        "val_targets = modif_final_df['Actual_Text'].to_numpy()\n",
        "\n",
        "f1_score_micro = metrics.f1_score(val_targets, val_preds, average='micro')\n",
        "f1_score_macro = metrics.f1_score(val_targets, val_preds, average='macro')\n",
        "\n",
        "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for label in list(id2label.values()):\n",
        "  copy_df = modif_final_df[final_df.Actual_Text == label]\n",
        "\n",
        "  test_preds = copy_df['Generated_Text'].to_numpy()\n",
        "  test_targets = copy_df['Actual_Text'].to_numpy()\n",
        "\n",
        "  f1_score_micro = metrics.f1_score(test_targets, test_preds, average='micro')\n",
        "  f1_score_macro = metrics.f1_score(test_targets, test_preds, average='macro')\n",
        "  print(f\"F1 Score (Micro) {label} = {f1_score_micro}\")\n",
        "  print(f\"F1 Score (Macro) {label} = {f1_score_macro}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "_YGesXLdKVwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences  = [\"you acknowledge and agree that , by accessing or using the site or services or by downloading or posting any content from or on the site or through the services , you are indicating that you have read , and that you understand and agree to be bound by , these terms , whether or not you have registered on or through the site . \", \"if we believe , in our sole discretion , that any member of academia.edu or academia premium or other academia.edu paid services is in breach of our terms , or act outside of the letter or spirit of our terms , we reserve the right to add limitations to your access to www.academia.edu , up to and including terminating all access to www.academia.edu . \", \"by accessing or using the site or services you represent and warrant that you are 13 years of age or older . \", 'in this case , the member in question is not eligible for any refunds on any portion of their subscription payment .']\n",
        "actual_sentences  = [\"Contract by using\", \"Unilateral change, Unilateral termination\", \"Contract by using\", 'No violation']\n",
        "\n",
        "for idx, i in enumerate(test_sentences):\n",
        "  translated_sentence = translate_sentence(model, i, SRC, TRG, device, max_length=50)\n",
        "  progress.append(TreebankWordDetokenizer().detokenize(translated_sentence))\n",
        "  # print(\"Text : {}\".format(i))\n",
        "  print(\"Actual Label: {}\".format(actual_sentences[idx]))\n",
        "  print(\"Predicted Label : {}\".format(progress[-1]))\n",
        "  print()"
      ],
      "metadata": {
        "id": "AAlaa-KAfnie"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Custom Seq2Seq Model for Machine Translation.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}